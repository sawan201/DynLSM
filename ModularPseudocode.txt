Files:
    gibbs.py
    initialize.py
    conditionalposteriors.py
    simulation.py
    diagnostics.py

gibbs.py:
    import conditionals as cds
    impot initialize as init

    class Gibbs:

        constructor(Y):
            Y is a T x n x n Numpy array of data
            self.Y = Y
            set self.T and self.n accordingly
        
        function RunGibbs(ns, p, modelType, initType,
                          nuIN, etaIN, nuOUT, etaOUT, thetaSigma, phiSigma, thetaTau, phiTau, alphas):
            Inputs: 
                ns (int number of steps)
                p (int dimension of latent space)
                modelType (either "poisson" or "binary")
                initialization (either "base" or something else that we create later)
                nuIN (mean of prior on betaIN)
                etaIN (variance of prior on betaIN)
                nuOUT (mean of prior on betaOUT)
                etaOUT (variance of prior on betaOUT)
                thetaSigma (shape parameter of prior on SigmaSq)
                phiSigma (scale parameter of prior on SigmaSq)
                thetaTau (shape parameter of prior on TauSq)
                phiTau (scale parameter of prior on TauSq)
                alphas (parameters for Dirichlet prior on r_{1:n})
                normalRWVariance (default value to 9)
                
                ** FOR NOW: 
                ** Fix the variance of the normal random walk (to the true value for sigma when testing)
                ** Fix the Dirichlet factor (what value we will use in the proposal)
            
            Outputs:
                X (ns x T x n x p Numpy array of latent positions samples from Markov Chain)
                r (ns x n Numpy array of reach samples from Markov Chain)
                tauSq (ns array of tauSq samples from Markov Chain)
                sigmaSq (ns array of sigmaSq samples from Markov Chain)
                betaIN (ns array of betaIN samples from Markov Chain)
                betaOUT (ns array of betaOUT samples from Markov Chain)
            
            ** Choose the right set of conditionals (the right class)
            if modelType == "binary":
                set conditionals = new object of class cds.BinaryConditionals()
                    pass in the following: nuIN, etaIN, nuOUT, etaOUT, thetaSigma, phiSigma, thetaTau, phiTau, alphas

            elif modeltype == "poisson":
                set conditionals = new object of class cds.PoissonConditionals()
                    pass in the following: nuIN, etaIN, nuOUT, etaOUT, thetaSigma, phiSigma, thetaTau, phiTau, alphas
            
            Determine n and T based on shape of Y
            Allocate the following empty Numpy arrays: name (shape)
                X (ns, T, n, p)
                r (ns, n)
                tauSq (ns)
                sigmaSq (ns)
                betaIN (ns)
                betaOUT (ns)
            
            ** Choose the right set of initializations
            if initType == "base":
                set initialization = new object of class init.BaseInitialization()

            Initialize values for X, r, tauSq, sigmaSq, betaIN, betaOUT
                Call a function on the initialization object that does this, namely InitializeAll()
            
            Initialize currentValue dictionary with:
                key : value
                "Y" : data (as T x n x n array)
                "X" : initial value of X (as T x n x n array)
                "r" : initial value of r (as n length array)
                "tauSq" : initial value of tauSq
                "sigmaSq" : initial value of sigmaSq
                "betaIN" : initial value of betaIN
                "betaOUT" : initial value of betaOUT
            
            Loop over iter in {1, 2, 3, ..., ns - 1}:
                
                ** Sampling Latent Positions
                Loop over t in {1, 2, ..., T}: (potentially time index {0, 1, 2, ..., T - 1})

                    if t == 1:
                        Set logPosterior = conditionals.LogTime1ConditionalPosterior()
                    
                    elif t == T:
                        Set logPosterior = conditionals.LogTimeTConditionalPosterior()

                    else:
                        Set logPosterior = conditionals.LogMiddleTimeConditionalPosterior()

                    Loop over i in {0, 1, 2, ..., n - 1}:

                        Call MetropolisHastings(logPosterior, conditionals.SampleFromNormalProposal, X[iter - 1, t, i], data)
                            where data has all the key-value pairs of currentState and the following:
                                "i" : i
                                "t" : t
                        Put returned value in proper place in X and in currentState   

                ** Sampling Radii
                Call MetropolisHastings(conditionals.LogRConditionalPosterior, SampleFromDirichletProposal, r[iter - 1], currentState
                                        logProposalEvaluate = LogEvaluateDirichlet, proposalSymmetric = False)
                Put returned value in proper place in r and in currentState

                ** Sampling Global Parameters with MH
                Call MetropolisHastings(conditionals.LogBetaINConditional, SampleFromNormalProposal, betaIN[iter - 1], currentState)
                Put returned value in proper place in betaIN and in currentState
                Call MetropolisHastings(conditionals.LogBetaOUTConditional, SampleFromNormalProposal, betaOUT[iter - 1], currentState)
                Put returned value in proper place in betaOUT and in currentState

                ** Directly Sampling Global Parameters
                Call conditionals.SampleTauSquared(currentState["X"])
                Put returned value in proper place in tauSq and in currentState
                Call conditionals.SampleSigmaSquared(currentState["X"])
                Put returned value in proper place in sigmaSq and in currentState

            return X, r, tauSq, sigmaSq, betaIN, betaOUT

        function MetropolisHastings(conditionalPosterior, proposalSampler, currentValue, data, 
                        logProposalEvaluate = None, proposalSymmetric = True, logPosterior = True):
            Inputs:
                conditionalPosterior: one of the conditional posterior functions below
                proposalSampler : one of the proposal sampler functions below
                currentValue : the current value in the Markov chain of the parameter under study
                data : dictionary of all values needed by the conditionalPosterior, proposalSampler
                logProposalEvaluate : one of the "evaluate at" functions below (only needed for asymmetric proposal, default None)
                proposalSymmetric : is the proposal symmetric? (False for Normal, True for Dirichlet)
                logPosterior : are we passing in log functions everywhere? (defaults to True)
            
            Output:
                next value in the Markov chain (either proposalValue or currentValue)

            # data = dictionary of necessary values to be passed into

            # Implements Metropolis-Hastings algorithm
            Sample from proposal distribution (pass in currentValue)
            Evaluate conditional posterior function at proposalValue (pass in data)
            Evaluate conditional posterior function at currentValue (pass in data)
            if proposalSymmetric == True:
                Calculate acceptance ratio (r), add/subtract if logPosterior = True (divide else)
            else:
                Evaluate proposal distribution at proposalValue given currentValue
                Evaluate proposal distribution at currentValue given proposalValue
                Calculate acceptance ratio (r), add/subtract if logPosterior = True (divide else)
            if r == 1:
                return proposalValue
            else:
                Sample from uniform distribution on [0, 1] -> uniformSample
                if uniformSample < r:
                    return proposalValue
                else:
                    return currentValue

        ** "SAMPLE FROM" FUNCTIONS (to be used as proposalSampler)
        Define a closure earlier in the function
        function SampleFromIndMultivariateNormalProposal(currentValue, variance = 1)
            return random sample from normal distribution centered at currentValue
                Fixed variance (should eventually be able to vary)

        function SampleFromDirichletProposal(currentValue, dirichletFactor = 10)
            return random sample from Dirichlet distribution with parameters dirichletFactor*currentValue
                (where dirichletFactor is a fixed number unless passed in)

        ** "EVALUATE AT" FUNCTIONS (to be used as proposalEvaluate for asymmetric proposals)
        function LogEvaluateDirichlet(parameters, values):
            Evaluate the log-probability of the Dirichlet distribution with specified parameters at values

== END gibbs.py == 

== BEGIN initialize.py ==
    Abstract class: AbstractInitialization
    class AbstractInitialization:
        constructor(Y, X, r, betaIN, betaOUT, sigmaSq, tauSq):
            set attributes to the parameters passed in

        function InitializeAll():
            InitializeX()
            InitializeR()
            InitializeBetaIN()
            InitializeBetaOUT()
            InitializeSigmaSq()
            InitializeTauSq()

        function InitializeX():
            raise error

        function InitializeR():
            raise error

        function InitializeTauSq():
            raise error

        function InitializeSigmaSq():
            raise error

        function InitializeBetaIN():
            raise error

        function InitializeBetaOUT():
            raise error

    Implementation of abstract class: BaseInitialization
    class BaseInitialization(AbstractInitialization):
        function InitializeX(X):
            For now, initialize X to all zeros across the entire tensor
            return X

        function InitializeR(r):
            For now, initalize r to 1/n, where n = length of r
            return r

        function InitializeTauSq(tauSq):
            For now, initialize tauSq = 1
            return tauSq

        function InitializeSigmaSq(sigmaSq):
            For now, initialize sigmaSq = 1
            return sigmaSq

        function InitializeBetaIN(betaIN):
            For now, initialize betaIN = 1
            return betaIN

        function InitializeBetaOUT(betaOUT):
            For now, initialize betaOUT = 1
            return betaOUT
== END initialize.py

== BEGIN conditionalposteriors.py ==    
    class ConditionalPosteriors:
        ConditionalPosteriors is a base class that will be inherited by others (like BinaryConditionals)
        
        ** CONDITIONAL POSTERIORS
        function LogTime1ConditionalPosterior(currentData, xValue, index, time):

            Set value proposedX to currentData["X"] but with the xValue at the correct time, index position
            Call self.LogLikelihood(currentData["Y"], proposedX, currentData["r"], currentData["betaIN"], 
                                    currentData["betaOUT"], currentData["tauSq"], currentData["sigmaSq"])
            Call self.LogX1Prior(proposedX, currentData["r"], currentData["betaIN"], currentData["betaOUT"],
                                 currentData["tauSq"], currentData["sigmaSq"])
            return the sum of the results above

        function LogMiddleTimeConditionalPosterior(currentData, xValue):
            Set value proposedX to currentData["X"] but with the xValue at the correct time, index position
            Call self.LogLikelihood(currentData["Y"], proposedX, currentData["r"], currentData["betaIN"], 
                                    currentData["betaOUT"], currentData["tauSq"], currentData["sigmaSq"])
            Call self.LogMiddleXPrior(proposedX, currentData["r"], currentData["betaIN"], currentData["betaOUT"],
                                      currentData["tauSq"], currentData["sigmaSq"])
            return the sum of the results above

        function LogTimeTConditionalPosterior(currentData, xValue):
            Set value proposedX to currentData["X"] but with the xValue at the correct time, index position
            Call self.LogLikelihood(currentData["Y"], proposedX, currentData["r"], currentData["betaIN"], 
                                    currentData["betaOUT"], currentData["tauSq"], currentData["sigmaSq"])
            Call self.LogXTPrior(proposedX, currentData["r"], currentData["betaIN"], currentData["betaOUT"],
                                 currentData["tauSq"], currentData["sigmaSq"])
            return the sum of the results above

        function LogRConditionalPosterior(currentData, rValues):
            Call self.LogLikelihood(currentData["Y"], currentData["X"], rValues, currentData["betaIN"], 
                                    currentData["betaOUT"], currentData["tauSq"], currentData["sigmaSq"])
            By the supplemental materials, R has a flat prior that does not affect the conditional posterior,
                so no function is needed here.
            return the sum of the results above
        
        function LogBetaINConditionalPosterior(currentData, betaINValue):
            Call self.LogLikelihood(currentData["Y"], currentData["X"], currentData["r"], betaINValue, 
                                    currentData["betaOUT"], currentData["tauSq"], currentData["sigmaSq"])
            Call self.LogBetaINPrior(currentData["X"], currentData["r"], betaINValue, currentData["betaOUT"],
                                currentData["tauSq"], currentData["sigmaSq"])
            return the sum of the results above

        function LogBetaOUTConditionalPosterior(currentData, betaOUTValue):
            Call self.LogLikelihood(currentData["Y"], currentData["X"], currentData["r"], currentData["betaIN"], 
                                    betaOUTValue, currentData["tauSq"], currentData["sigmaSq"])
            Call self.LogBetaOUTPrior(currentData["X"], currentData["r"], currentData["betaIN"], betaOUTValue,
                                     currentData["tauSq"], currentData["sigmaSq"])
            return the sum of the results above

        ** PRIOR FUNCTIONS
        function LogX1Prior(X, r, betaIN, betaOUT, tauSq, sigmaSq):
            Evaluate log-prior for X at t = 1 at the given values

        function LogMiddleXPrior(X, r, betaIN, betaOUT, tauSq, sigmaSq):
            Evaluate log-prior for X between t = 2 and t = T - 1 at the given values

        function LogXTPrior(X, r, betaIN, betaOUT, tauSq, sigmaSq):
            Evaluate log-prior for X at time t = T at the given values
        
        function LogRPrior(X, r, betaIN, betaOUT, tauSq, sigmaSq):
            Evaluate log-prior for R at the given values
        
        function LogBetaINPrior(X, r, betaIN, betaOUT, tauSq, sigmaSq):
            Evaluate log-prior for betaIN at the given values
        
        function LogBetaOUTPrior(X, r, betaIN, betaOUT, tauSq, sigmaSq):
            Evaluate log-prior for betaOUT at the given values

        ** LIKELIHOOD FUNCTION
        function LogLikelihood(Y, X, r, betaIN, betaOUT, tauSq, sigmaSq):
            Evaluates log-likelihood across the entire Y matrix (individual entries assumed to be independent)

            Inputs:
                X (shape T x n x n)
                r (shape n)
                betaIN, betaOUT, tauSq, sigmaSq (scalars)
            
            Output:
                log-likelihood of Y given everything else

            logLikelihood = 0
            T = X.shape[0]
            n = X.shape[1]

            for t in range(T):
                for j in range(n):
                    for i in range(n):
                        if i == j: 
                            continue
                        else:
                            Call logPijt(Y, X, r, betaIN, betaOUT, tauSq, sigmaSq) to calculate for Y[t, i, j]
                            Add result to logLikelihood variable
            
            return logLikelihood
        
        function LogPijt(Y, X, r, betaIN, betaOUT, tauSq, sigmaSq, i, j, t):
            print error message (we should never be using this function from the base class)
        
        function ETAijt(Y, X, r, betaIN, betaOUT, tauSq, sigmaSq, i, j, t):
            calculate and return eta using the typical formula at position i, j, t
        
        ** DIRECTLY SAMPLING FROM FUNCTIONS
        function SampleTauSquared(X):
            Calculate shape parameter of Tau^2 given else
            Calculate scale parameter of Tau^2 given else
            Sample from inverse gamma given shape, scale parameters
            return sample value
        
        function SampleSigmaSquared(X):
            Calculate shape parameter of Sigma^2 given else
            Calculate scale parameter of Sigma^2 given else
            Sample from inverse gamma given shape, scale parameters
            return sample value

    class BernoulliConditionals: inherits from ConditionalPosteriors
        constructor __init__(nuIN, etaIN, nuOUT, etaOUT, thetaSigma, phiSigma, thetaTau, phiTau, alphas, n, p, T):
            self.nuIN = nuIN
            self.etaIN = etaIN
            self.nuOUT = nuOUT
            self.etaOUT = etaOUT
            self.thetaSigma = thetaSigma
            self.phiSigma = phiSigma
            self.thetaTau = thetaTau
            self.phiTau = phiTau
            self.alphas = alphas
            self.n = n
            self.p = p
            self.T = T

        function LogPijt(Y, X, r, betaIN, betaOUT, tauSq, sigmaSq, i, j, t):
            Calculate log-likelihood of Y[t, i, j] given else under Bernoulli assumption using ETAijt

== END conditionalposteriors.py ==

== BEGIN diagnostics.py ==
GOAL: We need to construct certain plots for the simulations/models after building them.

class diagnostics:
    constructor(simResults, outPath, conditionals, truth = ""):
        Inputs:
            simResultsPath = path to .npz file containing tensors
                "Y", "X_Chain", "R_Chain", "betaIN_Chain", "betaOUT_Chain", "tauSq_Chain", "sigmaSq_Chain"
            outputPath = where to put the resulting plots/analyses
            conditionals = instance of conditionals used in the model for this
            truth = dictionary (for a simulation study only) of the key-value pairs:
                "trueX" : T x n x p tensor with true positions
                "trueR" : n-length vector with true radii
                "trueBetaIN" : true Beta_IN parameter used to generate data
                "trueBetaOUT" : true Beta_OUT parameter used to generate data
                "trueSigmaSq" : true sigma squared parameter used to generate data
                "trueTauSq" : true tau squared parameter used to generate data
        
        set self.Y, self.XChain, self.RChain, self.betaINChain, self.betaOUTChain, self.tauSqChain, self.sigmaSqChain,
            self.ns, self.T, self.n, self.p, self.outPath
        if knownTruth != "":
            set self.trueX, self.trueR, self.trueBetaIN, self.trueBetaOUT, self.trueSigmaSq, self.trueTauSq
    
    BuildGlobalTracePlots(self, showTruth = False):
        For all global variables, build and output (to self.outPath) a trace plot for betaIN, betaOUT, SigmaSq, TauSq
        If showTruth == True, plot the true values as well

    BuildGlobalHistograms(self, showTruth = False, binMethod = "sturges", burnIn = 0):
        For all global variables, build and output (to self.outPath) histograms for the distribution using the 
            specified bin-determining method (and ignoring anything in the burn-in period)
        If showTruth == True, plot the true values as well
    
    BuildPositionMarkovChainPlot(self, i, showTruth = False):
        For the index-i actor, plot its position over time for each iteration of the MCMC algorithm.
        (for the p = 1 case, plot in 2D; for the p = 2 case, plot in 3D)
        If showTruth = True, plot the true values as well
        Output result to self.outPath
    
    BuildPositionDynamicPlot(self, i, showTruth = False, burnIn = 0):
        For the index-i actor, plot the estimate of the actor's latent position over time indices 0 ... T
            (the average of the MCMC estimates after the specified burnIn)
        If showTruth == True, plot the true values as well
        Output result to self.outPath
    
    BuildLogLikelihoodPlot(self, conditionals, thinning = 1)
        (require an instance of the conditionals class to determine what log-likelihood to use)
        Determine the indices to plot the log-likelihood at (as determined by thinning argument)
            (it's costly to compute log-likelihood; this should speed it up)
        Calculate the log-likelihood at each index required using the conditionals instance
        Plot the log-likelihoods against the iteration
        Output result to self.outPath
    
    BuildParameterEstimates(self, burnIn = 0, showTruth = False)
        For each parameter, determine the average value for the Markov chain after burnIn number of steps
        (a scalar value for global parameters, a n-length vector for R, and a T x n x p tensor for positions)
        If showTruth == True, include the true values and the difference between estimated and true
        Output these results to a .txt file (or similar, maybe .npz?)
    
    BuildGlobalAutoCorrelationPlots(self, burnIn, maximumLag = self.ns):
        For each global parameter, build an autocorrelation plot where the maximumLag is as specified and the specified
            burnIn period is removed.

== END diagnostics.py ==